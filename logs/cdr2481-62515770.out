created virtual environment CPython3.12.4.final.0-64 in 8864ms
  creator CPython3Posix(dest=/localscratch/rmfrost.62515770.0/env, clear=False, no_vcs_ignore=False, global=False)
  seeder FromAppData(download=False, pip=bundle, via=copy, app_data_dir=/home/rmfrost/.local/share/virtualenv)
    added seed packages: pip==25.1.1
  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/torch-2.6.0+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/torchvision-0.21.0+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 2))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/pandas-2.2.3+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/numpy-2.2.2+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 4))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/matplotlib-3.10.0+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/thop-0.1.1.post2209072238+computecanada-py3-none-any.whl (from -r requirements.txt (line 6))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/scikit_learn-1.6.1+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/torchinfo-1.8.0+computecanada-py3-none-any.whl (from -r requirements.txt (line 8))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/filelock-3.18.0+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/typing_extensions-4.13.2+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/setuptools-80.8.0+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/sympy-1.13.1+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/networkx-3.5+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/jinja2-3.1.6+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/fsspec-2025.5.1+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/mpmath-1.3.0+computecanada-py3-none-any.whl (from sympy==1.13.1->torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/Pillow_SIMD-9.5.0.post2+computecanada-cp312-cp312-linux_x86_64.whl (from torchvision->-r requirements.txt (line 2))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/python_dateutil-2.9.0.post0+computecanada-py2.py3-none-any.whl (from pandas->-r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pytz-2025.2+computecanada-py2.py3-none-any.whl (from pandas->-r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tzdata-2025.2+computecanada-py2.py3-none-any.whl (from pandas->-r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/contourpy-1.3.1+computecanada-cp312-cp312-linux_x86_64.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/cycler-0.12.1+computecanada-py3-none-any.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/fonttools-4.58.1+computecanada-cp312-cp312-linux_x86_64.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/kiwisolver-1.4.8+computecanada-cp312-cp312-linux_x86_64.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/packaging-24.2+computecanada-py3-none-any.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/pillow-11.1.0+computecanada-cp312-cp312-linux_x86_64.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pyparsing-3.2.1+computecanada-py3-none-any.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/scipy-1.15.1+computecanada-cp312-cp312-linux_x86_64.whl (from scikit-learn->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/joblib-1.4.2+computecanada-py3-none-any.whl (from scikit-learn->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/threadpoolctl-3.6.0+computecanada-py3-none-any.whl (from scikit-learn->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/six-1.17.0+computecanada-py2.py3-none-any.whl (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/MarkupSafe-2.1.5+computecanada-cp312-cp312-linux_x86_64.whl (from jinja2->torch->-r requirements.txt (line 1))
Installing collected packages: pytz, mpmath, tzdata, typing-extensions, threadpoolctl, sympy, six, setuptools, pyparsing, pillow-simd, pillow, packaging, numpy, networkx, MarkupSafe, kiwisolver, joblib, fsspec, fonttools, filelock, cycler, scipy, python-dateutil, jinja2, contourpy, torch, scikit-learn, pandas, matplotlib, torchvision, thop, torchinfo

Successfully installed MarkupSafe-2.1.5+computecanada contourpy-1.3.1+computecanada cycler-0.12.1+computecanada filelock-3.18.0+computecanada fonttools-4.58.1+computecanada fsspec-2025.5.1+computecanada jinja2-3.1.6+computecanada joblib-1.4.2+computecanada kiwisolver-1.4.8+computecanada matplotlib-3.10.0+computecanada mpmath-1.3.0+computecanada networkx-3.5+computecanada numpy-2.2.2+computecanada packaging-24.2+computecanada pandas-2.2.3+computecanada pillow-11.1.0+computecanada pillow-simd-9.5.0.post2+computecanada pyparsing-3.2.1+computecanada python-dateutil-2.9.0.post0+computecanada pytz-2025.2+computecanada scikit-learn-1.6.1+computecanada scipy-1.15.1+computecanada setuptools-80.8.0+computecanada six-1.17.0+computecanada sympy-1.13.1+computecanada thop-0.1.1.post2209072238+computecanada threadpoolctl-3.6.0+computecanada torch-2.6.0+computecanada torchinfo-1.8.0+computecanada torchvision-0.21.0+computecanada typing-extensions-4.13.2+computecanada tzdata-2025.2+computecanada
cdr2481.int.cedar.computecanada.ca
 Static hostname: cdr2481.int.cedar.computecanada.ca
       Icon name: computer-server
         Chassis: server ðŸ–³
      Machine ID: 135352b42ec4476fb2414f72a0620478
         Boot ID: 2a85980fed8b4dfcb706a5851a5dd357
Operating System: AlmaLinux 9.3 (Shamrock Pampas Cat)
     CPE OS Name: cpe:/o:almalinux:almalinux:9::baseos
          Kernel: Linux 5.14.0-362.24.2.el9_3.x86_64
    Architecture: x86-64
 Hardware Vendor: Dell Inc.
  Hardware Model: PowerEdge C4140
Firmware Version: 2.18.1
Mon Jun  2 11:48:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             43W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   36C    P0             43W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  |   00000000:86:00.0 Off |                    0 |
| N/A   38C    P0             41W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   40C    P0             43W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Job ID: 62515770
Allocated GPUs: 0,1,2,3
Running on: cdr2481.int.cedar.computecanada.ca
Starting at: Mon Jun  2 11:48:03 PDT 2025
starting training...

Training model: small_dilation_last_two
Starting training at 2025-06-02 11:48:07
Using device: cuda
Training for 1000 epochs
Model: small_dilation_last_two
Dataset: majmin
Number of classes: 28
Loss hit epochs: 50
Early stop epochs: 200
Setting up data
Label distribution: Counter({np.str_('N'): 54716, np.str_('C:maj'): 44978, np.str_('G:maj'): 40613, np.str_('F:maj'): 40217, np.str_('D:maj'): 37902, np.str_('A:maj'): 34889, np.str_('E:maj'): 30587, np.str_('Bb:maj'): 21331, np.str_('Eb:maj'): 17877, np.str_('Ab:maj'): 17768, np.str_('A:min'): 14686, np.str_('B:maj'): 13947, np.str_('D:min'): 13846, np.str_('E:min'): 13432, np.str_('B:min'): 11867, np.str_('Db:maj'): 11757, np.str_('G:min'): 8302, np.str_('C:min'): 6837, np.str_('F:min'): 5921, np.str_('Gb:maj'): 5832, np.str_('Eb:min'): 5336, np.str_('Bb:min'): 3595, np.str_('Ab:min'): 1558, np.str_('Cb:maj'): 709, np.str_('Db:min'): 636, np.str_('Fb:maj'): 203, np.str_('Gb:min'): 151, np.str_('Cb:min'): 7})
Setting up network
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
MACs: 3,262,592.0
FLOPs: 6,525,184.0
GFLOPs: 0.0065
Parameters: 1009618.00
Epoch 1, Training Loss: 1.8168616129938053, Validation Loss: 1.5191782171679737
Epoch 2, Training Loss: 1.460483092949583, Validation Loss: 1.4926846686511983
Epoch 3, Training Loss: 1.4042637193767473, Validation Loss: 1.471100667891064
Epoch 4, Training Loss: 1.371029716818667, Validation Loss: 1.4651737639830968
Epoch 5, Training Loss: 1.3490475701477755, Validation Loss: 1.457302427806562
Epoch 6, Training Loss: 1.330004472226707, Validation Loss: 1.4560526253287174
Epoch 7, Training Loss: 1.31795109920515, Validation Loss: 1.4455090069671195
Epoch 8, Training Loss: 1.3069001702711112, Validation Loss: 1.4377187798448259
Epoch 9, Training Loss: 1.295000332339339, Validation Loss: 1.4392407915054257
Epoch 10, Training Loss: 1.287632956976346, Validation Loss: 1.4319193693588703
Epoch 11, Training Loss: 1.2794208901561623, Validation Loss: 1.4249833536679366
Epoch 12, Training Loss: 1.2725994188473417, Validation Loss: 1.4222696719063357
Epoch 13, Training Loss: 1.2657680590338694, Validation Loss: 1.413089285072842
Epoch 14, Training Loss: 1.259195847874343, Validation Loss: 1.4067110764615052
Epoch 15, Training Loss: 1.25418300017673, Validation Loss: 1.4076793142347948
Epoch 16, Training Loss: 1.2485249144619222, Validation Loss: 1.3982259501653793
Epoch 17, Training Loss: 1.2438190344482412, Validation Loss: 1.3989686343331191
Epoch 18, Training Loss: 1.2390710325736964, Validation Loss: 1.3994027476622866
Epoch 19, Training Loss: 1.2339870230733185, Validation Loss: 1.3862649924575785
Epoch 20, Training Loss: 1.2291149303607622, Validation Loss: 1.3892919534096146
Epoch 21, Training Loss: 1.224815658014354, Validation Loss: 1.3884616878537415
Epoch 22, Training Loss: 1.2211549898874128, Validation Loss: 1.3861903023586963
Epoch 23, Training Loss: 1.216913023394573, Validation Loss: 1.3843961686809083
Epoch 24, Training Loss: 1.2132435237630828, Validation Loss: 1.3745574405764471
Epoch 25, Training Loss: 1.2100506155105686, Validation Loss: 1.3750387775532715
Epoch 26, Training Loss: 1.205100878727248, Validation Loss: 1.375986582887538
Epoch 27, Training Loss: 1.2021901032220685, Validation Loss: 1.3739351413542182
Epoch 28, Training Loss: 1.1979184938783212, Validation Loss: 1.365660156961271
Epoch 29, Training Loss: 1.194288274399528, Validation Loss: 1.363981901603157
Epoch 30, Training Loss: 1.1916235661550927, Validation Loss: 1.3666072672954177
Epoch 31, Training Loss: 1.1886585021915541, Validation Loss: 1.3651791695598772
Epoch 32, Training Loss: 1.185609976533617, Validation Loss: 1.3627332211536949
Epoch 33, Training Loss: 1.1811718066312034, Validation Loss: 1.3619767594304257
Epoch 34, Training Loss: 1.1787596563276805, Validation Loss: 1.3606507660286673
Epoch 35, Training Loss: 1.175981896833457, Validation Loss: 1.3621940997151611
Epoch 36, Training Loss: 1.172801482339866, Validation Loss: 1.3582357295210314
Epoch 37, Training Loss: 1.1698698398130518, Validation Loss: 1.3548019828902647
Epoch 38, Training Loss: 1.166152067807613, Validation Loss: 1.3510221793292956
Epoch 39, Training Loss: 1.1637372650420543, Validation Loss: 1.3497139569942667
Epoch 40, Training Loss: 1.1611453483197682, Validation Loss: 1.353160601140397
Epoch 41, Training Loss: 1.159407210272556, Validation Loss: 1.3490741503603942
Epoch 42, Training Loss: 1.154811921862396, Validation Loss: 1.3436368093683195
Epoch 43, Training Loss: 1.1528356622354565, Validation Loss: 1.3426306085832271
Epoch 44, Training Loss: 1.149325139647628, Validation Loss: 1.3413832412622766
Epoch 45, Training Loss: 1.1474629816293938, Validation Loss: 1.3475737579187643
Epoch 46, Training Loss: 1.1440859723389978, Validation Loss: 1.342280618161544
Epoch 47, Training Loss: 1.142845576215698, Validation Loss: 1.3429143345787664
Epoch 48, Training Loss: 1.139133152080667, Validation Loss: 1.3459749988028598
Epoch 49, Training Loss: 1.1361782571288759, Validation Loss: 1.3358548513528032
Epoch 50, Training Loss: 1.1337129702253705, Validation Loss: 1.3308156970317648
Epoch 51, Training Loss: 1.1325552930904963, Validation Loss: 1.3355848290295986
Epoch 52, Training Loss: 1.1301206889369473, Validation Loss: 1.3393514446063295
Epoch 53, Training Loss: 1.1265783312703683, Validation Loss: 1.3340368470109605
Epoch 54, Training Loss: 1.124175457707576, Validation Loss: 1.3287196087970043
Epoch 55, Training Loss: 1.1211762176195337, Validation Loss: 1.3311719373076074
Epoch 56, Training Loss: 1.1183866516184564, Validation Loss: 1.3290785491300492
Epoch 57, Training Loss: 1.1164141706326767, Validation Loss: 1.3299621398402455
Epoch 58, Training Loss: 1.1141675665850095, Validation Loss: 1.3325074264432062
Epoch 59, Training Loss: 1.1108818727449454, Validation Loss: 1.3297866934853344
Epoch 60, Training Loss: 1.1089037706039522, Validation Loss: 1.3287199743112814
Epoch 61, Training Loss: 1.106985409888495, Validation Loss: 1.3285575963992595
Epoch 62, Training Loss: 1.1045156794504203, Validation Loss: 1.3286646821206658
Epoch 63, Training Loss: 1.1010847307513352, Validation Loss: 1.327434882065048
Epoch 64, Training Loss: 1.1000009867031484, Validation Loss: 1.328092346782472
Epoch 65, Training Loss: 1.0972737050654164, Validation Loss: 1.3219378351501103
Epoch 66, Training Loss: 1.0954467378065558, Validation Loss: 1.3270274165779103
Epoch 67, Training Loss: 1.0917602845553236, Validation Loss: 1.3197119871885996
Epoch 68, Training Loss: 1.0907160935018942, Validation Loss: 1.32346548957745
Epoch 69, Training Loss: 1.088078626074388, Validation Loss: 1.3268735911022653
Epoch 70, Training Loss: 1.0863155025293567, Validation Loss: 1.3213425731924584
Epoch 71, Training Loss: 1.0820951672372934, Validation Loss: 1.3254697954588281
Epoch 72, Training Loss: 1.0809011631191274, Validation Loss: 1.32515650331808
Epoch 73, Training Loss: 1.0784725928428334, Validation Loss: 1.325951909952509
Epoch 74, Training Loss: 1.0765284310088958, Validation Loss: 1.3221991096200385
Epoch 75, Training Loss: 1.0739898470229536, Validation Loss: 1.3198019139115857
Epoch 76, Training Loss: 1.071179940839996, Validation Loss: 1.3198423079295412
Epoch 77, Training Loss: 1.0698174672095777, Validation Loss: 1.3238544416792877
Epoch 78, Training Loss: 1.0677145383318154, Validation Loss: 1.3229538027124486
Epoch 79, Training Loss: 1.0648919731904316, Validation Loss: 1.3194320114708213
Epoch 80, Training Loss: 1.062151210470784, Validation Loss: 1.3232682182928315
Epoch 81, Training Loss: 1.0596233197905558, Validation Loss: 1.3222055146289071
Epoch 82, Training Loss: 1.0583173491496563, Validation Loss: 1.3215428657186397
Epoch 83, Training Loss: 1.0560011993765057, Validation Loss: 1.3201697627315947
Epoch 84, Training Loss: 1.0536263298966204, Validation Loss: 1.3244313721371228
Epoch 85, Training Loss: 1.0518825258938604, Validation Loss: 1.3198497783506813
Epoch 86, Training Loss: 1.0495794219282233, Validation Loss: 1.3158444040831085
Epoch 87, Training Loss: 1.047887128015968, Validation Loss: 1.3242759931220318
Epoch 88, Training Loss: 1.044181716475314, Validation Loss: 1.3204254626729695
Epoch 89, Training Loss: 1.0422337548867795, Validation Loss: 1.3180918295071318
Epoch 90, Training Loss: 1.0413694162001295, Validation Loss: 1.3210557246772692
Epoch 91, Training Loss: 1.037491044736949, Validation Loss: 1.317790581904414
Epoch 92, Training Loss: 1.0361507792326734, Validation Loss: 1.3228313441396091
Epoch 93, Training Loss: 1.0325225622375032, Validation Loss: 1.328526011416507
Epoch 94, Training Loss: 1.032223824671052, Validation Loss: 1.316537533878948
Epoch 95, Training Loss: 1.0293542462692509, Validation Loss: 1.323347241018476
Epoch 96, Training Loss: 1.0273883515413758, Validation Loss: 1.321193426349369
Epoch 97, Training Loss: 1.0250109770891283, Validation Loss: 1.3205957959787427
Epoch 98, Training Loss: 1.0239184742850513, Validation Loss: 1.3200079830244058
Epoch 99, Training Loss: 1.021470899397063, Validation Loss: 1.3216895767407164
Epoch 100, Training Loss: 1.0198606933391858, Validation Loss: 1.3173213428441528
Epoch 101, Training Loss: 1.0157281116633472, Validation Loss: 1.3261176717978667
Epoch 102, Training Loss: 1.0153230901326753, Validation Loss: 1.3235193639578593
Epoch 103, Training Loss: 1.0109825634580025, Validation Loss: 1.3286224923260033
Epoch 104, Training Loss: 1.0121038356309924, Validation Loss: 1.321231700915812
Epoch 105, Training Loss: 1.0093889415485502, Validation Loss: 1.3213199168692724
Epoch 106, Training Loss: 1.0080723509040272, Validation Loss: 1.3268565438418003
Epoch 107, Training Loss: 1.0032633921285414, Validation Loss: 1.321077784206874
Epoch 108, Training Loss: 1.0022667015218691, Validation Loss: 1.3269943136690718
Epoch 109, Training Loss: 0.999969620115799, Validation Loss: 1.3275673661723442
Epoch 110, Training Loss: 0.9979417146967867, Validation Loss: 1.3244426872238808
Epoch 111, Training Loss: 0.9971256351293849, Validation Loss: 1.3279756162325984
Epoch 112, Training Loss: 0.9928705399525641, Validation Loss: 1.3280303063996988
Epoch 113, Training Loss: 0.9924059216358535, Validation Loss: 1.3297997796103815
Epoch 114, Training Loss: 0.9894865947181468, Validation Loss: 1.3201363099152663
Epoch 115, Training Loss: 0.9865446821228707, Validation Loss: 1.3309462890651564
Epoch 116, Training Loss: 0.9848144002390218, Validation Loss: 1.3307634430177364
Epoch 117, Training Loss: 0.9829700075594938, Validation Loss: 1.3300222220693125
Epoch 118, Training Loss: 0.9807828990197779, Validation Loss: 1.3343435896140288
Epoch 119, Training Loss: 0.9784640413860314, Validation Loss: 1.3308369442613013
Epoch 120, Training Loss: 0.976111638236068, Validation Loss: 1.3335650126582068
Epoch 121, Training Loss: 0.9744491919999667, Validation Loss: 1.3281075057877139
Epoch 122, Training Loss: 0.9727807614187898, Validation Loss: 1.3343561464365479
Epoch 123, Training Loss: 0.9704279261489875, Validation Loss: 1.3312589382560804
Epoch 124, Training Loss: 0.9701358465929217, Validation Loss: 1.3296346711747162
Epoch 125, Training Loss: 0.9670113054213971, Validation Loss: 1.3361100785247462
Epoch 126, Training Loss: 0.9655605964673927, Validation Loss: 1.331802775351782
Epoch 127, Training Loss: 0.9621087916769578, Validation Loss: 1.3304542423456824
Epoch 128, Training Loss: 0.9605308890398055, Validation Loss: 1.3381420556549242
Epoch 129, Training Loss: 0.9563418084145036, Validation Loss: 1.3342882188083733
Epoch 130, Training Loss: 0.9580303610422707, Validation Loss: 1.3360649930899522
Epoch 131, Training Loss: 0.9531756321257093, Validation Loss: 1.3437994796585573
Epoch 132, Training Loss: 0.9526841906950002, Validation Loss: 1.3392237771022286
Epoch 133, Training Loss: 0.9505135740188946, Validation Loss: 1.3343208712787682
Epoch 134, Training Loss: 0.9498769804185037, Validation Loss: 1.3399606161130837
Epoch 135, Training Loss: 0.9468799708834819, Validation Loss: 1.342442628151859
Weight Optimization Hit
Epoch 136, Training Loss: 0.9347497324148814, Validation Loss: 1.3433943128519403
Epoch 137, Training Loss: 0.9335794090228493, Validation Loss: 1.3426218087626698
Epoch 138, Training Loss: 0.9335999798475866, Validation Loss: 1.3429338858984308
Epoch 139, Training Loss: 0.9307314824095011, Validation Loss: 1.3411020716252766
Epoch 140, Training Loss: 0.9313322251719685, Validation Loss: 1.3526159028488947
Epoch 141, Training Loss: 0.9293178651160184, Validation Loss: 1.3443601303279897
Epoch 142, Training Loss: 0.9277276722722952, Validation Loss: 1.3495285078509605
Epoch 143, Training Loss: 0.9266140884066912, Validation Loss: 1.3483308321254166
Epoch 144, Training Loss: 0.9250329198832853, Validation Loss: 1.3488194667695292
Epoch 145, Training Loss: 0.9229271314214974, Validation Loss: 1.3504572108927544
Epoch 146, Training Loss: 0.923696411131927, Validation Loss: 1.3500482527990527
Epoch 147, Training Loss: 0.9219113303592278, Validation Loss: 1.3518464072169036
Epoch 148, Training Loss: 0.9223142299202712, Validation Loss: 1.3522392059767148
Epoch 149, Training Loss: 0.9210199164702257, Validation Loss: 1.3456874700641899
Epoch 150, Training Loss: 0.917958870488621, Validation Loss: 1.3503072554852638
Epoch 151, Training Loss: 0.9167159828413607, Validation Loss: 1.3536061621309987
Epoch 152, Training Loss: 0.916415487685022, Validation Loss: 1.3543339304439204
Epoch 153, Training Loss: 0.9143372178354414, Validation Loss: 1.353585592883543
Epoch 154, Training Loss: 0.9146696172772233, Validation Loss: 1.3536142940972842
Epoch 155, Training Loss: 0.9132514729639282, Validation Loss: 1.3565866297998137
Epoch 156, Training Loss: 0.9109608063348046, Validation Loss: 1.356607538577906
Epoch 157, Training Loss: 0.9118092466308324, Validation Loss: 1.3591734928008905
Epoch 158, Training Loss: 0.9094185771882368, Validation Loss: 1.3599687374733949
Epoch 159, Training Loss: 0.9098838217687917, Validation Loss: 1.3577758509634597
Epoch 160, Training Loss: 0.907237676192791, Validation Loss: 1.3603890511650893
Epoch 161, Training Loss: 0.9076068111614929, Validation Loss: 1.3561411055847794
Epoch 162, Training Loss: 0.9059481901932117, Validation Loss: 1.3583198675372141
Epoch 163, Training Loss: 0.9032559137767182, Validation Loss: 1.3621252355137243
Epoch 164, Training Loss: 0.9028173006842728, Validation Loss: 1.3671448880749493
Epoch 165, Training Loss: 0.9025268198830614, Validation Loss: 1.3630914476422546
Epoch 166, Training Loss: 0.9014374675192873, Validation Loss: 1.3638536433655573
Epoch 167, Training Loss: 0.8996366337149256, Validation Loss: 1.3625778184793786
Epoch 168, Training Loss: 0.8989450788763572, Validation Loss: 1.3616682604826924
Epoch 169, Training Loss: 0.8983565877739101, Validation Loss: 1.36365283000436
Epoch 170, Training Loss: 0.8974273878330419, Validation Loss: 1.36696603520667
Epoch 171, Training Loss: 0.8959263513900665, Validation Loss: 1.3601429769587716
Epoch 172, Training Loss: 0.8956411004675243, Validation Loss: 1.366870741741239
Epoch 173, Training Loss: 0.8931833382768551, Validation Loss: 1.3698075845381013
Epoch 174, Training Loss: 0.8936874045745686, Validation Loss: 1.369162053284871
Epoch 175, Training Loss: 0.8925617439058388, Validation Loss: 1.3719772370246792
Epoch 176, Training Loss: 0.8897884392362009, Validation Loss: 1.3657203741724444
Epoch 177, Training Loss: 0.8897806224218651, Validation Loss: 1.3736474633548916
Epoch 178, Training Loss: 0.8892365873081328, Validation Loss: 1.3709074603315847
Epoch 179, Training Loss: 0.888411501988063, Validation Loss: 1.36777540659506
Epoch 180, Training Loss: 0.8874192212119408, Validation Loss: 1.3717680265478438
Epoch 181, Training Loss: 0.8853613575410267, Validation Loss: 1.3700215781631575
Epoch 182, Training Loss: 0.8847803531029541, Validation Loss: 1.3755444807926593
Epoch 183, Training Loss: 0.8849113590205945, Validation Loss: 1.3760888843815307
Epoch 184, Training Loss: 0.883065674575037, Validation Loss: 1.3721331241735177
Weight Optimization Hit
Epoch 185, Training Loss: 0.8774363352340354, Validation Loss: 1.3704562334131065
Epoch 186, Training Loss: 0.8751533422463451, Validation Loss: 1.3731795257180515
Epoch 187, Training Loss: 0.8763921220018117, Validation Loss: 1.375285449134274
Epoch 188, Training Loss: 0.8745549817879995, Validation Loss: 1.373804660975767
Epoch 189, Training Loss: 0.8741992589435649, Validation Loss: 1.3758206533522328
Epoch 190, Training Loss: 0.874361981865202, Validation Loss: 1.374889868796702
Epoch 191, Training Loss: 0.8729369923984329, Validation Loss: 1.3781044254894044
Epoch 192, Training Loss: 0.8724148331578395, Validation Loss: 1.3773692762453245
Epoch 193, Training Loss: 0.8730139756850214, Validation Loss: 1.3812308429343454
Epoch 194, Training Loss: 0.871946997048131, Validation Loss: 1.3800904937607332
Epoch 195, Training Loss: 0.8708891102917901, Validation Loss: 1.377534535518928
Epoch 196, Training Loss: 0.8703262571006765, Validation Loss: 1.3823820531036197
Epoch 197, Training Loss: 0.870345214200219, Validation Loss: 1.3838957355547084
Epoch 198, Training Loss: 0.8688921927575614, Validation Loss: 1.3799976603566437
Epoch 199, Training Loss: 0.8684215042031022, Validation Loss: 1.3804593020660965
Epoch 200, Training Loss: 0.8677561803061097, Validation Loss: 1.383029410683013
Epoch 201, Training Loss: 0.8678561249538606, Validation Loss: 1.377114226808123
Epoch 202, Training Loss: 0.8682600455419156, Validation Loss: 1.382968644411783
Epoch 203, Training Loss: 0.8666913997496071, Validation Loss: 1.3821668361720933
Epoch 204, Training Loss: 0.8661459542416596, Validation Loss: 1.3799901513336097
Epoch 205, Training Loss: 0.8645710133382758, Validation Loss: 1.3818028289461535
Epoch 206, Training Loss: 0.8651601778431524, Validation Loss: 1.379376899482148
Epoch 207, Training Loss: 0.8647135358473274, Validation Loss: 1.3821976883829803
Epoch 208, Training Loss: 0.8653984482131186, Validation Loss: 1.3840203950996186
Epoch 209, Training Loss: 0.8634023260162623, Validation Loss: 1.3821477715683514
Epoch 210, Training Loss: 0.8619254661193022, Validation Loss: 1.386250292939396
Epoch 211, Training Loss: 0.8634216028889131, Validation Loss: 1.386432069506818
Epoch 212, Training Loss: 0.8602116196766537, Validation Loss: 1.3863033235571178
Epoch 213, Training Loss: 0.8611111380401981, Validation Loss: 1.3847098529836925
Epoch 214, Training Loss: 0.859711854598427, Validation Loss: 1.3851624537975342
Epoch 215, Training Loss: 0.8585704844254748, Validation Loss: 1.3864059838411869
Epoch 216, Training Loss: 0.8606875205383105, Validation Loss: 1.3896054489366856
Epoch 217, Training Loss: 0.858173977780807, Validation Loss: 1.3845373708391588
Epoch 218, Training Loss: 0.8582593507255353, Validation Loss: 1.3904416554485524
Epoch 219, Training Loss: 0.8587399623464852, Validation Loss: 1.3891280176413756
Epoch 220, Training Loss: 0.8564907574720038, Validation Loss: 1.3884290050829353
Epoch 221, Training Loss: 0.8574016770778592, Validation Loss: 1.388257690790968
Epoch 222, Training Loss: 0.8560133001100386, Validation Loss: 1.3883714293204974
Epoch 223, Training Loss: 0.8578846207789613, Validation Loss: 1.3900118085833313
Epoch 224, Training Loss: 0.8547791946841924, Validation Loss: 1.387871975022106
Epoch 225, Training Loss: 0.8543600308031757, Validation Loss: 1.389120400865098
Epoch 226, Training Loss: 0.8556899185134176, Validation Loss: 1.3927641876228674
Epoch 227, Training Loss: 0.8537834466001781, Validation Loss: 1.3935280713696334
Epoch 228, Training Loss: 0.8537714137949098, Validation Loss: 1.3945165875064298
Epoch 229, Training Loss: 0.8544036579718718, Validation Loss: 1.3944690592773779
Epoch 230, Training Loss: 0.8528332432996596, Validation Loss: 1.3903554795014161
Epoch 231, Training Loss: 0.8531715828119436, Validation Loss: 1.3900573809000776
Epoch 232, Training Loss: 0.8495173270545188, Validation Loss: 1.3970612268427953
Epoch 233, Training Loss: 0.8511713762026533, Validation Loss: 1.3915785693856666
Weight Optimization Hit
Epoch 234, Training Loss: 0.8465532759963746, Validation Loss: 1.398517544970207
Epoch 235, Training Loss: 0.8460931405494251, Validation Loss: 1.3932837109213752
Epoch 236, Training Loss: 0.8460531401047579, Validation Loss: 1.399434127515405
Epoch 237, Training Loss: 0.8487643755854781, Validation Loss: 1.3950741284404957
Epoch 238, Training Loss: 0.8467753542554302, Validation Loss: 1.3963152056450963
Epoch 239, Training Loss: 0.8456702791188974, Validation Loss: 1.3983965822415099
Epoch 240, Training Loss: 0.8459877068230479, Validation Loss: 1.3981871933658143
Epoch 241, Training Loss: 0.8437064448051577, Validation Loss: 1.3987357037811226
Epoch 242, Training Loss: 0.8446602596933352, Validation Loss: 1.3985336799807535
Epoch 243, Training Loss: 0.8447344026968451, Validation Loss: 1.4009137079742293
Epoch 244, Training Loss: 0.8442685826575191, Validation Loss: 1.3986239839231072
Epoch 245, Training Loss: 0.8446753297205202, Validation Loss: 1.4004662020292786
Epoch 246, Training Loss: 0.8441560291646251, Validation Loss: 1.4015142923444095
Epoch 247, Training Loss: 0.8455904189811997, Validation Loss: 1.3985154894401104
Epoch 248, Training Loss: 0.8443729211416307, Validation Loss: 1.4007221485246855
Epoch 249, Training Loss: 0.843220585817082, Validation Loss: 1.4000795338313228
Epoch 250, Training Loss: 0.8420830809250959, Validation Loss: 1.4001310396327282
Epoch 251, Training Loss: 0.8424109184393311, Validation Loss: 1.4020732617145792
Epoch 252, Training Loss: 0.8432644554444121, Validation Loss: 1.3983679503451483
Epoch 253, Training Loss: 0.8421647390117221, Validation Loss: 1.4010569580252124
Epoch 254, Training Loss: 0.8401799454053686, Validation Loss: 1.3996663436418124
Epoch 255, Training Loss: 0.8419444483939209, Validation Loss: 1.4034044237854089
Epoch 256, Training Loss: 0.8417203012670537, Validation Loss: 1.4048275489328963
Epoch 257, Training Loss: 0.8405717335980362, Validation Loss: 1.4028956906709167
Epoch 258, Training Loss: 0.8404555743285208, Validation Loss: 1.4018450629246269
Epoch 259, Training Loss: 0.8401698905390064, Validation Loss: 1.403604729759992
Epoch 260, Training Loss: 0.8414630835656225, Validation Loss: 1.4020169110849374
Epoch 261, Training Loss: 0.839536398275649, Validation Loss: 1.4012452021116666
Epoch 262, Training Loss: 0.8398877554395847, Validation Loss: 1.4082039299947637
Epoch 263, Training Loss: 0.8389087131760579, Validation Loss: 1.406540620708864
Epoch 264, Training Loss: 0.8385719365562238, Validation Loss: 1.4037579139461092
Epoch 265, Training Loss: 0.8397676677148211, Validation Loss: 1.4018837671758073
Epoch 266, Training Loss: 0.8381307752130202, Validation Loss: 1.4039928600980711
Epoch 267, Training Loss: 0.8380212542655409, Validation Loss: 1.4064680298888916
Epoch 268, Training Loss: 0.839871991754353, Validation Loss: 1.4026121649901515
Epoch 269, Training Loss: 0.8368756126339167, Validation Loss: 1.4051630114280413
Epoch 270, Training Loss: 0.8378465330036238, Validation Loss: 1.4062207273952143
Epoch 271, Training Loss: 0.8362200694606607, Validation Loss: 1.4038614032992414
Epoch 272, Training Loss: 0.838015208005241, Validation Loss: 1.406756430864334
Epoch 273, Training Loss: 0.8365599292511174, Validation Loss: 1.4061419998702898
Epoch 274, Training Loss: 0.8366238258730624, Validation Loss: 1.4041028536627884
Epoch 275, Training Loss: 0.8373558061091902, Validation Loss: 1.4052468159736697
Epoch 276, Training Loss: 0.8373884041080236, Validation Loss: 1.410955682023322
Epoch 277, Training Loss: 0.8364374002263628, Validation Loss: 1.4056072695009556
Epoch 278, Training Loss: 0.835591770286569, Validation Loss: 1.4040219882570601
Epoch 279, Training Loss: 0.8364187092170078, Validation Loss: 1.408333440760052
Epoch 280, Training Loss: 0.836011757785342, Validation Loss: 1.4070696901312114
Epoch 281, Training Loss: 0.8338706053952543, Validation Loss: 1.4042679955534285
Epoch 282, Training Loss: 0.836215995840154, Validation Loss: 1.40487956253599
Weight Optimization Hit
Epoch 283, Training Loss: 0.834743004696947, Validation Loss: 1.4065523145092562
Epoch 284, Training Loss: 0.8313288717299783, Validation Loss: 1.4056807035855263
Epoch 285, Training Loss: 0.8317151078219311, Validation Loss: 1.405834276945146
Ending Training Early
Loss plot saved to: ModelResults/small_dilation_last_two/majmin/loss_plot_majmin_classification.png
Accuracy: 0.1536, F1 Score: 0.1422
Model statistics saved to: ModelResults/small_dilation_last_two/majmin/model_stats_majmin.txt
Model saved to ModelResults/small_dilation_last_two/majmin/model.pth

Training completed at 2025-06-02 12:46:20
Total execution time: 3492.89 seconds (58.21 minutes)
