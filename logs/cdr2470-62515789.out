created virtual environment CPython3.12.4.final.0-64 in 12422ms
  creator CPython3Posix(dest=/localscratch/rmfrost.62515789.0/env, clear=False, no_vcs_ignore=False, global=False)
  seeder FromAppData(download=False, pip=bundle, via=copy, app_data_dir=/home/rmfrost/.local/share/virtualenv)
    added seed packages: pip==25.1.1
  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/torch-2.6.0+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/torchvision-0.21.0+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 2))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/pandas-2.2.3+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/numpy-2.2.2+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 4))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/matplotlib-3.10.0+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/thop-0.1.1.post2209072238+computecanada-py3-none-any.whl (from -r requirements.txt (line 6))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/scikit_learn-1.6.1+computecanada-cp312-cp312-linux_x86_64.whl (from -r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/torchinfo-1.8.0+computecanada-py3-none-any.whl (from -r requirements.txt (line 8))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/filelock-3.18.0+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/typing_extensions-4.13.2+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/setuptools-80.8.0+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/sympy-1.13.1+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/networkx-3.5+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/jinja2-3.1.6+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/fsspec-2025.5.1+computecanada-py3-none-any.whl (from torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/mpmath-1.3.0+computecanada-py3-none-any.whl (from sympy==1.13.1->torch->-r requirements.txt (line 1))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/Pillow_SIMD-9.5.0.post2+computecanada-cp312-cp312-linux_x86_64.whl (from torchvision->-r requirements.txt (line 2))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/python_dateutil-2.9.0.post0+computecanada-py2.py3-none-any.whl (from pandas->-r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pytz-2025.2+computecanada-py2.py3-none-any.whl (from pandas->-r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tzdata-2025.2+computecanada-py2.py3-none-any.whl (from pandas->-r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/contourpy-1.3.1+computecanada-cp312-cp312-linux_x86_64.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/cycler-0.12.1+computecanada-py3-none-any.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/fonttools-4.58.1+computecanada-cp312-cp312-linux_x86_64.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/kiwisolver-1.4.8+computecanada-cp312-cp312-linux_x86_64.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/packaging-24.2+computecanada-py3-none-any.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/pillow-11.1.0+computecanada-cp312-cp312-linux_x86_64.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pyparsing-3.2.1+computecanada-py3-none-any.whl (from matplotlib->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/scipy-1.15.1+computecanada-cp312-cp312-linux_x86_64.whl (from scikit-learn->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/joblib-1.4.2+computecanada-py3-none-any.whl (from scikit-learn->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/threadpoolctl-3.6.0+computecanada-py3-none-any.whl (from scikit-learn->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/six-1.17.0+computecanada-py2.py3-none-any.whl (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/MarkupSafe-2.1.5+computecanada-cp312-cp312-linux_x86_64.whl (from jinja2->torch->-r requirements.txt (line 1))
Installing collected packages: pytz, mpmath, tzdata, typing-extensions, threadpoolctl, sympy, six, setuptools, pyparsing, pillow-simd, pillow, packaging, numpy, networkx, MarkupSafe, kiwisolver, joblib, fsspec, fonttools, filelock, cycler, scipy, python-dateutil, jinja2, contourpy, torch, scikit-learn, pandas, matplotlib, torchvision, thop, torchinfo

Successfully installed MarkupSafe-2.1.5+computecanada contourpy-1.3.1+computecanada cycler-0.12.1+computecanada filelock-3.18.0+computecanada fonttools-4.58.1+computecanada fsspec-2025.5.1+computecanada jinja2-3.1.6+computecanada joblib-1.4.2+computecanada kiwisolver-1.4.8+computecanada matplotlib-3.10.0+computecanada mpmath-1.3.0+computecanada networkx-3.5+computecanada numpy-2.2.2+computecanada packaging-24.2+computecanada pandas-2.2.3+computecanada pillow-11.1.0+computecanada pillow-simd-9.5.0.post2+computecanada pyparsing-3.2.1+computecanada python-dateutil-2.9.0.post0+computecanada pytz-2025.2+computecanada scikit-learn-1.6.1+computecanada scipy-1.15.1+computecanada setuptools-80.8.0+computecanada six-1.17.0+computecanada sympy-1.13.1+computecanada thop-0.1.1.post2209072238+computecanada threadpoolctl-3.6.0+computecanada torch-2.6.0+computecanada torchinfo-1.8.0+computecanada torchvision-0.21.0+computecanada typing-extensions-4.13.2+computecanada tzdata-2025.2+computecanada
cdr2470.int.cedar.computecanada.ca
 Static hostname: cdr2470.int.cedar.computecanada.ca
       Icon name: computer-server
         Chassis: server ðŸ–³
      Machine ID: 135352b42ec4476fb2414f72a0620478
         Boot ID: e571074a4c4a416198f3acc8ada1c64f
Operating System: AlmaLinux 9.3 (Shamrock Pampas Cat)
     CPE OS Name: cpe:/o:almalinux:almalinux:9::baseos
          Kernel: Linux 5.14.0-362.24.2.el9_3.x86_64
    Architecture: x86-64
 Hardware Vendor: Dell Inc.
  Hardware Model: PowerEdge C4140
Firmware Version: 2.18.1
Mon Jun  2 19:03:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             41W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0             44W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  |   00000000:86:00.0 Off |                    0 |
| N/A   37C    P0             41W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   37C    P0             44W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Job ID: 62515789
Allocated GPUs: 0,1,2,3
Running on: cdr2470.int.cedar.computecanada.ca
Starting at: Mon Jun  2 19:03:26 PDT 2025
starting training...

Training model: early_squeeze
Starting training at 2025-06-02 19:03:31
Using device: cuda
Training for 1000 epochs
Model: early_squeeze
Dataset: majmin
Number of classes: 28
Loss hit epochs: 50
Early stop epochs: 200
Setting up data
Label distribution: Counter({np.str_('N'): 54716, np.str_('C:maj'): 44978, np.str_('G:maj'): 40613, np.str_('F:maj'): 40217, np.str_('D:maj'): 37902, np.str_('A:maj'): 34889, np.str_('E:maj'): 30587, np.str_('Bb:maj'): 21331, np.str_('Eb:maj'): 17877, np.str_('Ab:maj'): 17768, np.str_('A:min'): 14686, np.str_('B:maj'): 13947, np.str_('D:min'): 13846, np.str_('E:min'): 13432, np.str_('B:min'): 11867, np.str_('Db:maj'): 11757, np.str_('G:min'): 8302, np.str_('C:min'): 6837, np.str_('F:min'): 5921, np.str_('Gb:maj'): 5832, np.str_('Eb:min'): 5336, np.str_('Bb:min'): 3595, np.str_('Ab:min'): 1558, np.str_('Cb:maj'): 709, np.str_('Db:min'): 636, np.str_('Fb:maj'): 203, np.str_('Gb:min'): 151, np.str_('Cb:min'): 7})
Setting up network
/localscratch/rmfrost.62515789.0/env/lib/python3.12/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
MACs: 3,620,288.0
FLOPs: 7,240,576.0
GFLOPs: 0.0072
Parameters: 1011202.00
Epoch 1, Training Loss: 1.429288032603906, Validation Loss: 2.5733162324076577
Epoch 2, Training Loss: 1.275188410127174, Validation Loss: 2.586604818992296
Epoch 3, Training Loss: 1.2394015516466639, Validation Loss: 2.6146921248157042
Epoch 4, Training Loss: 1.216088570315415, Validation Loss: 2.6216471294838737
Epoch 5, Training Loss: 1.1985678287048305, Validation Loss: 2.6358586863554954
Epoch 6, Training Loss: 1.1842639640789954, Validation Loss: 2.6131152628524057
Epoch 7, Training Loss: 1.1721300530345105, Validation Loss: 2.5931026135314474
Epoch 8, Training Loss: 1.1619054464798009, Validation Loss: 2.6909477420171988
Epoch 9, Training Loss: 1.1520066198200236, Validation Loss: 2.6859990632301587
Epoch 10, Training Loss: 1.1431594630193134, Validation Loss: 2.636031409492068
Epoch 11, Training Loss: 1.13532712098173, Validation Loss: 2.676264469337995
Epoch 12, Training Loss: 1.127591198699388, Validation Loss: 2.6483985384858753
Epoch 13, Training Loss: 1.1195315905710892, Validation Loss: 2.7103524993389096
Epoch 14, Training Loss: 1.1130847339289214, Validation Loss: 2.6657953650838486
Epoch 15, Training Loss: 1.106838028272657, Validation Loss: 2.6811841102031613
Epoch 16, Training Loss: 1.100405147010348, Validation Loss: 2.6520846119166084
Epoch 17, Training Loss: 1.093930531704769, Validation Loss: 2.7177133659798454
Epoch 18, Training Loss: 1.088115886808438, Validation Loss: 2.693165753711233
Epoch 19, Training Loss: 1.0819675489056408, Validation Loss: 2.7179753249070107
Epoch 20, Training Loss: 1.0770868253076973, Validation Loss: 2.703913654789619
Epoch 21, Training Loss: 1.0710431750493683, Validation Loss: 2.728154289523207
Epoch 22, Training Loss: 1.0658593299550487, Validation Loss: 2.714347803493064
Epoch 23, Training Loss: 1.0603832851822108, Validation Loss: 2.7437088472929507
Epoch 24, Training Loss: 1.0550415065959304, Validation Loss: 2.7742631461959033
Epoch 25, Training Loss: 1.0508209981626122, Validation Loss: 2.795810366075351
Epoch 26, Training Loss: 1.045566104580765, Validation Loss: 2.7667290496958996
Epoch 27, Training Loss: 1.0397603459787679, Validation Loss: 2.7835771064572348
Epoch 28, Training Loss: 1.0356497041528272, Validation Loss: 2.780726589864343
Epoch 29, Training Loss: 1.0303497319987447, Validation Loss: 2.7796332022605834
Epoch 30, Training Loss: 1.0265595162756043, Validation Loss: 2.858981379227386
Epoch 31, Training Loss: 1.0221104514521366, Validation Loss: 2.8380962503985776
Epoch 32, Training Loss: 1.0162091323260145, Validation Loss: 2.7609199979511145
Epoch 33, Training Loss: 1.0127505115203095, Validation Loss: 2.8074345369524942
Epoch 34, Training Loss: 1.008034227396895, Validation Loss: 2.7813428218650285
Epoch 35, Training Loss: 1.0024363038544315, Validation Loss: 2.799115021580773
Epoch 36, Training Loss: 0.998971464989245, Validation Loss: 2.7881757635260027
Epoch 37, Training Loss: 0.9950770445742204, Validation Loss: 2.8075985124848346
Epoch 38, Training Loss: 0.9900581380395614, Validation Loss: 2.817343673334148
Epoch 39, Training Loss: 0.9863950924510301, Validation Loss: 2.8627815940585974
Epoch 40, Training Loss: 0.9827308923476475, Validation Loss: 2.8291505973651216
Epoch 41, Training Loss: 0.9780096937742295, Validation Loss: 2.8394915801901006
Epoch 42, Training Loss: 0.9734228848580863, Validation Loss: 2.8198735694035184
Epoch 43, Training Loss: 0.9698607847440431, Validation Loss: 2.8452616093218492
Epoch 44, Training Loss: 0.9655429943911766, Validation Loss: 2.8550729170483136
Epoch 45, Training Loss: 0.9618286618836632, Validation Loss: 2.8455785416626997
Epoch 46, Training Loss: 0.9582197282639718, Validation Loss: 2.8589125685705117
Epoch 47, Training Loss: 0.9532720367999241, Validation Loss: 2.854323303467052
Epoch 48, Training Loss: 0.9501050187961415, Validation Loss: 2.8936613147305246
Epoch 49, Training Loss: 0.9454669754817293, Validation Loss: 2.894448783736375
Epoch 50, Training Loss: 0.9414527622106016, Validation Loss: 2.864926685529831
Weight Optimization Hit
Epoch 51, Training Loss: 0.929433472309272, Validation Loss: 2.8622238287354578
Epoch 52, Training Loss: 0.9255640140649004, Validation Loss: 2.8853511561258256
Epoch 53, Training Loss: 0.9235961355594097, Validation Loss: 2.8556068049831973
Epoch 54, Training Loss: 0.9219269165864341, Validation Loss: 2.8750527270325046
Epoch 55, Training Loss: 0.9190097294035285, Validation Loss: 2.8818815053339457
Epoch 56, Training Loss: 0.9187501666716325, Validation Loss: 2.8876440455322476
Epoch 57, Training Loss: 0.9149909178305248, Validation Loss: 2.8759516998917944
Epoch 58, Training Loss: 0.9135051541231026, Validation Loss: 2.8942975459988736
Epoch 59, Training Loss: 0.9101785599299903, Validation Loss: 2.9012968423970893
Epoch 60, Training Loss: 0.9088111044415303, Validation Loss: 2.9373433154937616
Epoch 61, Training Loss: 0.9077023260274637, Validation Loss: 2.909023766398098
Epoch 62, Training Loss: 0.9065964576427651, Validation Loss: 2.9058605923958143
Epoch 63, Training Loss: 0.9034105175163532, Validation Loss: 2.8843690993726088
Epoch 64, Training Loss: 0.902113418697536, Validation Loss: 2.9178264330356565
Epoch 65, Training Loss: 0.900782031601186, Validation Loss: 2.913873497490099
Epoch 66, Training Loss: 0.8974911086572578, Validation Loss: 2.9332743650691424
Epoch 67, Training Loss: 0.8956854311038525, Validation Loss: 2.9103608214423518
Epoch 68, Training Loss: 0.8932701867824366, Validation Loss: 2.943790490248741
Epoch 69, Training Loss: 0.8926402698984385, Validation Loss: 2.9083448316393454
Epoch 70, Training Loss: 0.8898147650637224, Validation Loss: 2.9324012490036098
Epoch 71, Training Loss: 0.8886452582885296, Validation Loss: 2.9346653741050233
Epoch 72, Training Loss: 0.8870908048214908, Validation Loss: 2.94889354506575
Epoch 73, Training Loss: 0.8842880941975858, Validation Loss: 2.922210824854859
Epoch 74, Training Loss: 0.8817601710862325, Validation Loss: 2.9368101986005777
Epoch 75, Training Loss: 0.879968077663813, Validation Loss: 2.8980702463962906
Epoch 76, Training Loss: 0.8792333814593079, Validation Loss: 2.9313053855656914
Epoch 77, Training Loss: 0.8765279465288395, Validation Loss: 2.931940924822454
Epoch 78, Training Loss: 0.8749479914054675, Validation Loss: 2.9023946118554034
Epoch 79, Training Loss: 0.8728749220196361, Validation Loss: 2.9621430891137934
Epoch 80, Training Loss: 0.8718019013285305, Validation Loss: 2.9520551072521792
Epoch 81, Training Loss: 0.8686996702806974, Validation Loss: 2.9543552873526444
Epoch 82, Training Loss: 0.8668720396226938, Validation Loss: 2.9919554267421073
Epoch 83, Training Loss: 0.8664122522430721, Validation Loss: 2.950235217394603
Epoch 84, Training Loss: 0.8638127893624532, Validation Loss: 2.9799326103709864
Epoch 85, Training Loss: 0.8624351862247276, Validation Loss: 2.9387964475121673
Epoch 86, Training Loss: 0.8603324541474451, Validation Loss: 2.949417984585244
Epoch 87, Training Loss: 0.8585904162020848, Validation Loss: 3.0131889784236474
Epoch 88, Training Loss: 0.857517367950057, Validation Loss: 2.9694315048977526
Epoch 89, Training Loss: 0.8554178727315107, Validation Loss: 2.9857744662542527
Epoch 90, Training Loss: 0.8538215182783876, Validation Loss: 2.9894184285883783
Epoch 91, Training Loss: 0.8511551324703567, Validation Loss: 2.9838352063904234
Epoch 92, Training Loss: 0.84880730269015, Validation Loss: 2.9839534231547193
Epoch 93, Training Loss: 0.8487650732144008, Validation Loss: 2.998994435440531
Epoch 94, Training Loss: 0.8465359259946764, Validation Loss: 3.0154116392799737
Epoch 95, Training Loss: 0.8451990788195458, Validation Loss: 2.9671041158580516
Epoch 96, Training Loss: 0.8434461243139336, Validation Loss: 2.976121026493381
Epoch 97, Training Loss: 0.8417213328092764, Validation Loss: 2.9838576811601856
Epoch 98, Training Loss: 0.8396838724945249, Validation Loss: 3.034670022536785
Epoch 99, Training Loss: 0.838309771613933, Validation Loss: 3.0232531576103487
Weight Optimization Hit
Epoch 100, Training Loss: 0.831407263778157, Validation Loss: 3.0000954936141757
Epoch 101, Training Loss: 0.8288117511856855, Validation Loss: 3.0137562496084356
Epoch 102, Training Loss: 0.8285455308308066, Validation Loss: 3.0238566780488805
Epoch 103, Training Loss: 0.8287733690873715, Validation Loss: 3.0262756649830216
Epoch 104, Training Loss: 0.8260401847358091, Validation Loss: 3.022431600724754
Epoch 105, Training Loss: 0.8255937068741301, Validation Loss: 3.0257114450911624
Epoch 106, Training Loss: 0.8246238976301921, Validation Loss: 3.0206086100974123
Epoch 107, Training Loss: 0.8237762412431623, Validation Loss: 3.0467669565365507
Epoch 108, Training Loss: 0.8227975372650497, Validation Loss: 3.0367904313759553
Epoch 109, Training Loss: 0.8212352073790303, Validation Loss: 3.032113194465637
Epoch 110, Training Loss: 0.8208756888200535, Validation Loss: 3.0587813193419517
Epoch 111, Training Loss: 0.8206880632770648, Validation Loss: 3.0155170989899913
Epoch 112, Training Loss: 0.81913511996039, Validation Loss: 3.0344639315910658
Epoch 113, Training Loss: 0.8176077032044958, Validation Loss: 3.035302611114587
Epoch 114, Training Loss: 0.8171544192063112, Validation Loss: 3.028198111688194
Epoch 115, Training Loss: 0.8169467192395705, Validation Loss: 3.0325996726336255
Epoch 116, Training Loss: 0.8160748346851396, Validation Loss: 3.0572845872730268
Epoch 117, Training Loss: 0.8152792611632396, Validation Loss: 3.034651805763457
Epoch 118, Training Loss: 0.8138923390025659, Validation Loss: 3.0385718704266136
Epoch 119, Training Loss: 0.8138748738177971, Validation Loss: 3.017320922158223
Epoch 120, Training Loss: 0.8114060039252402, Validation Loss: 3.0548645865618353
Epoch 121, Training Loss: 0.8118120885714404, Validation Loss: 3.0483255064255017
Epoch 122, Training Loss: 0.8100927064235053, Validation Loss: 3.0668024883960947
Epoch 123, Training Loss: 0.8101514943822623, Validation Loss: 3.031874615833952
Epoch 124, Training Loss: 0.8098199347046203, Validation Loss: 3.068707749372073
Epoch 125, Training Loss: 0.8083521325276091, Validation Loss: 3.040833077722937
Epoch 126, Training Loss: 0.8071203757848802, Validation Loss: 3.063640035626616
Epoch 127, Training Loss: 0.8064867758374582, Validation Loss: 3.0434116437242555
Epoch 128, Training Loss: 0.805304662915104, Validation Loss: 3.069326287856673
Epoch 129, Training Loss: 0.804955024725326, Validation Loss: 3.088231422774971
Epoch 130, Training Loss: 0.8042977205032315, Validation Loss: 3.051518997110032
Epoch 131, Training Loss: 0.8034107519336177, Validation Loss: 3.069999227284721
Epoch 132, Training Loss: 0.8021871643893234, Validation Loss: 3.0876949600522563
Epoch 133, Training Loss: 0.8020027828858534, Validation Loss: 3.0846767027065947
Epoch 134, Training Loss: 0.7996310237224388, Validation Loss: 3.074731161999503
Epoch 135, Training Loss: 0.8004059586241606, Validation Loss: 3.0750253286866425
Epoch 136, Training Loss: 0.7997710355034335, Validation Loss: 3.0805226673986916
Epoch 137, Training Loss: 0.7989444724226175, Validation Loss: 3.0606458309965214
Epoch 138, Training Loss: 0.7975979338892102, Validation Loss: 3.0804364093498933
Epoch 139, Training Loss: 0.796241703513273, Validation Loss: 3.0508822863480507
Epoch 140, Training Loss: 0.7955693370950587, Validation Loss: 3.068764063309162
Epoch 141, Training Loss: 0.7956854154804623, Validation Loss: 3.0779351683380214
Epoch 142, Training Loss: 0.7935896096067951, Validation Loss: 3.088633456270011
Epoch 143, Training Loss: 0.7935348992394206, Validation Loss: 3.0909746998199847
Epoch 144, Training Loss: 0.7918630116995331, Validation Loss: 3.0578903527645016
Epoch 145, Training Loss: 0.7918690593295775, Validation Loss: 3.087731388618023
Epoch 146, Training Loss: 0.7911039576584693, Validation Loss: 3.100047433608754
Epoch 147, Training Loss: 0.7900986176402677, Validation Loss: 3.104051784220512
Epoch 148, Training Loss: 0.7895822185770494, Validation Loss: 3.0865721297463335
Weight Optimization Hit
Epoch 149, Training Loss: 0.7847451523236909, Validation Loss: 3.0869333219395374
Epoch 150, Training Loss: 0.7839208056695393, Validation Loss: 3.0993857878496387
Epoch 151, Training Loss: 0.7841409373692926, Validation Loss: 3.0897397543394467
Epoch 152, Training Loss: 0.7839955373450131, Validation Loss: 3.0915855610935137
Epoch 153, Training Loss: 0.7828101785568141, Validation Loss: 3.103486551547781
Epoch 154, Training Loss: 0.7832347190590401, Validation Loss: 3.0908127554943965
Epoch 155, Training Loss: 0.783138288110633, Validation Loss: 3.073746737331401
Epoch 156, Training Loss: 0.7812126775582632, Validation Loss: 3.122069796479844
Epoch 157, Training Loss: 0.7812941886256333, Validation Loss: 3.1004727627241513
Epoch 158, Training Loss: 0.7803989705130693, Validation Loss: 3.0957656222133583
Epoch 159, Training Loss: 0.7806791736222908, Validation Loss: 3.099921512072465
Epoch 160, Training Loss: 0.7800936587396992, Validation Loss: 3.0984459859747076
Epoch 161, Training Loss: 0.7791348952547532, Validation Loss: 3.096476736507044
Epoch 162, Training Loss: 0.7786768177038227, Validation Loss: 3.1082862854668023
Epoch 163, Training Loss: 0.7794415022669396, Validation Loss: 3.121298330740012
Epoch 164, Training Loss: 0.7778351515282496, Validation Loss: 3.110248538112906
Epoch 165, Training Loss: 0.7776758678970009, Validation Loss: 3.0957244269363065
Epoch 166, Training Loss: 0.7775651183301028, Validation Loss: 3.105012872092903
Epoch 167, Training Loss: 0.7774901191531892, Validation Loss: 3.106710922751254
Epoch 168, Training Loss: 0.7778453864260968, Validation Loss: 3.1158463108174317
Epoch 169, Training Loss: 0.7771545464696991, Validation Loss: 3.1124890212561094
Epoch 170, Training Loss: 0.7761851505150702, Validation Loss: 3.1129256579869304
Epoch 171, Training Loss: 0.776593533565075, Validation Loss: 3.1188400030800225
Epoch 172, Training Loss: 0.7751705289827858, Validation Loss: 3.1036911445739874
Epoch 173, Training Loss: 0.774468643544112, Validation Loss: 3.127603839366881
Epoch 174, Training Loss: 0.7737042500309737, Validation Loss: 3.1104030934548974
Epoch 175, Training Loss: 0.7734575038473144, Validation Loss: 3.094310943794782
Epoch 176, Training Loss: 0.7734843723704223, Validation Loss: 3.1105421393030532
Epoch 177, Training Loss: 0.7728737558799423, Validation Loss: 3.1286729202297074
Epoch 178, Training Loss: 0.7726318913831021, Validation Loss: 3.1209669850330832
Epoch 179, Training Loss: 0.7719997104772729, Validation Loss: 3.1189555982361266
Epoch 180, Training Loss: 0.7714599617925527, Validation Loss: 3.122976218757523
Epoch 181, Training Loss: 0.7722036993907576, Validation Loss: 3.1159056544635955
Epoch 182, Training Loss: 0.7712400798844096, Validation Loss: 3.1133098725156865
Epoch 183, Training Loss: 0.7713046676324492, Validation Loss: 3.108601539911998
Epoch 184, Training Loss: 0.7703096920013871, Validation Loss: 3.118321005016316
Epoch 185, Training Loss: 0.7701655338373911, Validation Loss: 3.123265073159943
Epoch 186, Training Loss: 0.7700179308210132, Validation Loss: 3.1024977855363596
Epoch 187, Training Loss: 0.7688607702182196, Validation Loss: 3.1138339165525517
Epoch 188, Training Loss: 0.769372842770101, Validation Loss: 3.106893007801768
Epoch 189, Training Loss: 0.7675156627773907, Validation Loss: 3.1538700356788953
Epoch 190, Training Loss: 0.7685317824782325, Validation Loss: 3.15058039423483
Epoch 191, Training Loss: 0.7667112371632647, Validation Loss: 3.134666880193195
Epoch 192, Training Loss: 0.7665476574041054, Validation Loss: 3.12982331477832
Epoch 193, Training Loss: 0.7656608460092279, Validation Loss: 3.1161020130832213
Epoch 194, Training Loss: 0.7661184804131836, Validation Loss: 3.1397127915889773
Epoch 195, Training Loss: 0.766493852299568, Validation Loss: 3.1320203037979213
Epoch 196, Training Loss: 0.7653832911974651, Validation Loss: 3.122035815855255
Epoch 197, Training Loss: 0.7646049556764277, Validation Loss: 3.134425177879652
Weight Optimization Hit
Epoch 198, Training Loss: 0.7619034527985167, Validation Loss: 3.1403567515375888
Epoch 199, Training Loss: 0.7621112770661227, Validation Loss: 3.126179910967941
Epoch 200, Training Loss: 0.7626500836986021, Validation Loss: 3.1351204174142695
Ending Training Early
Loss plot saved to: ModelResults/early_squeeze/majmin/loss_plot_majmin_classification.png
Accuracy: 0.5176, F1 Score: 0.4904
Model statistics saved to: ModelResults/early_squeeze/majmin/model_stats_majmin.txt
Model saved to ModelResults/early_squeeze/majmin/model.pth

Training completed at 2025-06-02 19:57:54
Total execution time: 3262.95 seconds (54.38 minutes)
